# 分布式图计算框架

## 项目背景
大规模图计算常用分布式并行方法来处理无法在单机内存中容纳的图数据。通过将大图划分为多个子图，每个进程/节点负责一部分，局部计算后通过消息传递（如MPI）同步边界信息。

## 核心架构

### 分布式图划分
```cpp
// 图划分示例结构
struct GraphPartition {
    vector<int> local_vertices;     // 本地顶点
    vector<Edge> local_edges;       // 本地边
    vector<int> boundary_vertices;  // 边界顶点
    unordered_map<int, int> vertex_to_process; // 顶点到进程的映射
};
```

### MPI并行BFS实现
```cpp
#include <mpi.h>
#include <vector>
#include <queue>

class DistributedBFS {
private:
    int rank, size;
    vector<vector<int>> local_graph;
    vector<int> local_distances;
    vector<bool> visited;
    
public:
    void parallel_bfs(int source) {
        queue<int> current_frontier;
        queue<int> next_frontier;
        
        // 初始化
        if (is_local_vertex(source)) {
            current_frontier.push(source);
            local_distances[source] = 0;
            visited[source] = true;
        }
        
        int level = 0;
        while (!current_frontier.empty()) {
            // 处理当前层
            while (!current_frontier.empty()) {
                int vertex = current_frontier.front();
                current_frontier.pop();
                
                // 遍历邻居
                for (int neighbor : local_graph[vertex]) {
                    if (!visited[neighbor]) {
                        visited[neighbor] = true;
                        local_distances[neighbor] = level + 1;
                        next_frontier.push(neighbor);
                    }
                }
            }
            
            // 跨进程通信：交换边界信息
            exchange_frontier_data(next_frontier);
            
            // 全局同步：检查是否所有进程都完成
            int local_empty = next_frontier.empty() ? 1 : 0;
            int global_empty;
            MPI_Allreduce(&local_empty, &global_empty, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
            
            if (global_empty == size) break; // 所有进程都没有新节点
            
            current_frontier = next_frontier;
            next_frontier = queue<int>();
            level++;
        }
    }
    
private:
    void exchange_frontier_data(queue<int>& frontier) {
        // 收集需要发送给其他进程的顶点信息
        vector<vector<int>> send_buffers(size);
        
        while (!frontier.empty()) {
            int vertex = frontier.front();
            frontier.pop();
            
            // 确定该顶点属于哪个进程
            int target_process = get_vertex_owner(vertex);
            if (target_process != rank) {
                send_buffers[target_process].push_back(vertex);
            } else {
                // 本地顶点，重新加入队列
                frontier.push(vertex);
            }
        }
        
        // MPI通信
        for (int i = 0; i < size; i++) {
            if (i != rank && !send_buffers[i].empty()) {
                MPI_Send(send_buffers[i].data(), send_buffers[i].size(), 
                        MPI_INT, i, 0, MPI_COMM_WORLD);
            }
        }
        
        // 接收来自其他进程的数据
        for (int i = 0; i < size; i++) {
            if (i != rank) {
                MPI_Status status;
                MPI_Probe(i, 0, MPI_COMM_WORLD, &status);
                int count;
                MPI_Get_count(&status, MPI_INT, &count);
                
                vector<int> recv_buffer(count);
                MPI_Recv(recv_buffer.data(), count, MPI_INT, i, 0, 
                        MPI_COMM_WORLD, &status);
                
                for (int vertex : recv_buffer) {
                    frontier.push(vertex);
                }
            }
        }
    }
};
```

## 关键挑战

### 1. 图划分的均衡性
- **负载均衡**：确保各进程的计算量大致相等
- **通信最小化**：减少跨进程的边数量
- **实现策略**：
  - Hash-based划分：`vertex_id % num_processes`
  - Range-based划分：连续顶点ID段分配
  - 高阶图划分算法：METIS、Scotch等

### 2. 通信开销优化
```cpp
// 批量通信优化
class CommunicationBuffer {
    vector<vector<Message>> send_queues;
    static const int BATCH_SIZE = 1000;
    
public:
    void add_message(int target_rank, const Message& msg) {
        send_queues[target_rank].push_back(msg);
        
        // 达到批量大小时发送
        if (send_queues[target_rank].size() >= BATCH_SIZE) {
            flush_to_rank(target_rank);
        }
    }
    
    void flush_all() {
        for (int i = 0; i < send_queues.size(); i++) {
            if (!send_queues[i].empty()) {
                flush_to_rank(i);
            }
        }
    }
};
```

### 3. 动态负载均衡
```cpp
class LoadBalancer {
public:
    void redistribute_work() {
        // 收集各进程的工作负载
        int local_workload = get_local_workload();
        vector<int> all_workloads(size);
        
        MPI_Allgather(&local_workload, 1, MPI_INT, 
                     all_workloads.data(), 1, MPI_INT, MPI_COMM_WORLD);
        
        // 计算平均负载
        int total_work = 0;
        for (int work : all_workloads) total_work += work;
        int average_work = total_work / size;
        
        // 重新分配过载的工作
        if (local_workload > average_work * 1.2) {
            migrate_vertices_to_underloaded_processes();
        }
    }
};
```

## 性能优化策略

### 异步通信
```cpp
class AsyncCommunicator {
    vector<MPI_Request> send_requests;
    vector<MPI_Request> recv_requests;
    
public:
    void async_send(const vector<int>& data, int target_rank) {
        MPI_Request request;
        MPI_Isend(data.data(), data.size(), MPI_INT, 
                 target_rank, 0, MPI_COMM_WORLD, &request);
        send_requests.push_back(request);
    }
    
    void wait_all_communications() {
        MPI_Waitall(send_requests.size(), send_requests.data(), MPI_STATUSES_IGNORE);
        MPI_Waitall(recv_requests.size(), recv_requests.data(), MPI_STATUSES_IGNORE);
    }
};
```

### 内存优化
```cpp
// 压缩图表示
struct CompressedGraph {
    vector<int> row_ptr;      // CSR格式行指针
    vector<int> col_indices;  // 列索引
    
    // 节省内存的邻居访问
    auto get_neighbors(int vertex) {
        return make_pair(
            col_indices.begin() + row_ptr[vertex],
            col_indices.begin() + row_ptr[vertex + 1]
        );
    }
};
```

## 图学习算法集成

### GCN集成示例
```cpp
class GraphNeuralNetwork {
public:
    void distributed_gcn_forward(const GraphPartition& partition) {
        // 特征聚合阶段
        aggregate_neighbor_features(partition);
        
        // 跨进程特征交换
        exchange_boundary_features();
        
        // 神经网络前向传播
        apply_neural_layers();
    }
    
private:
    void aggregate_neighbor_features(const GraphPartition& partition) {
        for (int vertex : partition.local_vertices) {
            vector<float> aggregated_features(feature_dim, 0.0f);
            
            for (int neighbor : get_neighbors(vertex)) {
                for (int i = 0; i < feature_dim; i++) {
                    aggregated_features[i] += node_features[neighbor][i];
                }
            }
            
            // 归一化
            float degree = get_degree(vertex);
            for (float& feature : aggregated_features) {
                feature /= sqrt(degree);
            }
            
            updated_features[vertex] = aggregated_features;
        }
    }
};
```

## 系统架构设计

### 模块化设计
```cpp
class DistributedGraphFramework {
private:
    unique_ptr<GraphPartitioner> partitioner;
    unique_ptr<CommunicationManager> comm_manager;
    unique_ptr<LoadBalancer> load_balancer;
    unique_ptr<AlgorithmRunner> algorithm_runner;
    
public:
    void initialize(int argc, char** argv) {
        MPI_Init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        MPI_Comm_size(MPI_COMM_WORLD, &size);
        
        partitioner = make_unique<GraphPartitioner>();
        comm_manager = make_unique<CommunicationManager>(rank, size);
        load_balancer = make_unique<LoadBalancer>();
        algorithm_runner = make_unique<AlgorithmRunner>();
    }
    
    void run_algorithm(const string& algorithm_name, const Graph& graph) {
        // 图划分
        auto partition = partitioner->partition_graph(graph, size);
        
        // 执行算法
        algorithm_runner->run(algorithm_name, partition);
        
        // 动态负载均衡（如需要）
        if (load_balancer->should_rebalance()) {
            load_balancer->redistribute_work();
        }
    }
};
```

## 核心价值
分布式图计算的核心是"分而治之"：
1. **可扩展性**：处理单机无法容纳的大规模图数据
2. **并行性**：充分利用集群的计算资源
3. **容错性**：通过检查点和恢复机制提升鲁棒性
4. **通用性**：支持多种图算法和图学习算法的高效执行

通过合理的图划分和高效的通信机制，可以在保证算法正确性的前提下，实现接近线性的性能扩展。 